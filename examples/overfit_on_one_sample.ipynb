{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit on one sample\n",
    "+ Step 1: Uncomment `dataset = dataset.take(1)` in `segmentation/datasets/dataset_utils.py`.\n",
    "+ Step 2: Modify dataset config.\n",
    "+ Step 3: Run the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from segmentation.builders import model_builder, dataset_builder\n",
    "from segmentation.utils import losses_utils, metrics_utils\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # multi-gpu configs\n",
    "    parser.add_argument('--num_gpus', type=int, default=1)\n",
    "    parser.add_argument('--gpu_devices', type=str, default=\"3\")\n",
    "\n",
    "    # dataset\n",
    "    parser.add_argument('--dataset_name', type=str, default=\"pascal_voc_seg\", help='')\n",
    "    parser.add_argument('--dataset_dir', type=str, default=\"/hdd02/zhangyiyang/data/VOCdevkit/segmentation_aug_tfrecords\", help='')\n",
    "    parser.add_argument('--split_name', type=str, default=\"val\", help='')\n",
    "    parser.add_argument('--eval_crop_height', type=int, default=513)\n",
    "    parser.add_argument('--eval_crop_width', type=int, default=513)\n",
    "    parser.add_argument('--max_resize_value', type=int, default=None)\n",
    "    parser.add_argument('--min_resize_value', type=int, default=None)\n",
    "\n",
    "    # model related\n",
    "    parser.add_argument('--model_type', type=str, default=\"deeplab_v3_plus\")\n",
    "    parser.add_argument('--backend_type', type=str, default=\"xception\")\n",
    "    parser.add_argument('--model_weights', type=str, default=\"pascal_voc\")\n",
    "    parser.add_argument('--output_stride', type=int, default=16)\n",
    "    parser.add_argument('--fine_tune_batch_norm', action='store_true',\n",
    "                        help='Whether to fine tune bach norm.')\n",
    "\n",
    "    return parser.parse_args([])\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_datasets(args):\n",
    "    val_dataset_configs = dataset_builder.build_dataset_configs(\n",
    "        dataset_dir=args.dataset_dir,\n",
    "        batch_size=1,\n",
    "        crop_size=(args.eval_crop_height, args.eval_crop_width),\n",
    "        max_resize_value=args.max_resize_value,\n",
    "        min_resize_value=args.min_resize_value,\n",
    "        should_shuffle=False,\n",
    "        is_training=False,\n",
    "        should_repeat=True,\n",
    "    )\n",
    "    val_dataset = dataset_builder.build_dataset(\n",
    "        args.dataset_name, args.split_name, True, val_dataset_configs)\n",
    "\n",
    "    return val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.__version__.split('.')[0] == \"1\":\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=tf_config)\n",
    "    tf.keras.backend.set_session(sess)\n",
    "\n",
    "dataset_meta = dataset_builder.build_dataset_meta(args.dataset_name)\n",
    "val_dataset = _get_datasets(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/zyy_tf1.14/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From ../segmentation/utils/metrics_utils.py:56: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
      "\n",
      "WARNING:tensorflow:From ../segmentation/utils/metrics_utils.py:70: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From ../segmentation/utils/metrics_utils.py:71: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "keras_model = model_builder.build_model(\n",
    "    model_type=args.model_type,\n",
    "    backend_type=args.backend_type,\n",
    "    weights=args.model_weights,\n",
    "    num_classes=dataset_meta.num_classes,\n",
    "    OS=args.output_stride,\n",
    "    input_shape=(args.eval_crop_height, args.eval_crop_width, 3),\n",
    "    fine_tune_batch_norm=args.fine_tune_batch_norm,\n",
    ")\n",
    "keras_model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-4),\n",
    "    loss=losses_utils.build_cross_entropy_loss_fn(dataset_meta.num_classes),\n",
    "    metrics=[\n",
    "        metrics_utils.build_accuracy_fn(dataset_meta.num_classes),\n",
    "        metrics_utils.build_mean_iou_fn(dataset_meta.num_classes)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.1588 - _accuracy: 0.9611 - _mean_iou: 0.3487\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0510 - _accuracy: 0.9832 - _mean_iou: 0.9455\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0797 - _accuracy: 0.9765 - _mean_iou: 0.6134\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0461 - _accuracy: 0.9755 - _mean_iou: 0.9243\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0216 - _accuracy: 0.9912 - _mean_iou: 0.9699\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0103 - _accuracy: 0.9982 - _mean_iou: 0.9937\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0076 - _accuracy: 0.9987 - _mean_iou: 0.9957\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0062 - _accuracy: 0.9991 - _mean_iou: 0.9970\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0053 - _accuracy: 0.9992 - _mean_iou: 0.9974\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0047 - _accuracy: 0.9993 - _mean_iou: 0.9975\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0042 - _accuracy: 0.9994 - _mean_iou: 0.9980\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0036 - _accuracy: 0.9995 - _mean_iou: 0.9982\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0035 - _accuracy: 0.9995 - _mean_iou: 0.9983\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0031 - _accuracy: 0.9996 - _mean_iou: 0.9985\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0029 - _accuracy: 0.9994 - _mean_iou: 0.9980\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0028 - _accuracy: 0.9996 - _mean_iou: 0.9987\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0027 - _accuracy: 0.9995 - _mean_iou: 0.9983\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0024 - _accuracy: 0.9997 - _mean_iou: 0.9989\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0022 - _accuracy: 0.9997 - _mean_iou: 0.9990\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0020 - _accuracy: 0.9998 - _mean_iou: 0.9992\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0019 - _accuracy: 0.9998 - _mean_iou: 0.9993\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.0019 - _accuracy: 0.9998 - _mean_iou: 0.9992\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0017 - _accuracy: 0.9998 - _mean_iou: 0.9992\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0016 - _accuracy: 0.9997 - _mean_iou: 0.9990\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0015 - _accuracy: 0.9997 - _mean_iou: 0.9991\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0014 - _accuracy: 0.9998 - _mean_iou: 0.9995\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0012 - _accuracy: 0.9999 - _mean_iou: 0.9996\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0011 - _accuracy: 1.0000 - _mean_iou: 0.9999\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0010 - _accuracy: 1.0000 - _mean_iou: 0.9998\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 9.4340e-04 - _accuracy: 1.0000 - _mean_iou: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6d0999ac18>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model.fit(val_dataset,\n",
    "               steps_per_epoch=1,\n",
    "               epochs=30,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyy_tf1.14",
   "language": "python",
   "name": "zyy_tf1.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
